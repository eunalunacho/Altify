import torch
from transformers import (
    AutoProcessor,
    LlavaForConditionalGeneration,
    BitsAndBytesConfig
)
from PIL import Image
from typing import Optional
import logging
import time

logger = logging.getLogger(__name__)


class LLaVAModelLoader:
    """Singleton íŒ¨í„´ì„ ì‚¬ìš©í•œ LLaVA ëª¨ë¸ ë¡œë”"""
    
    _instance = None
    _model = None
    _processor = None
    _is_loaded = False
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super(LLaVAModelLoader, cls).__new__(cls)
        return cls._instance
    
    def load(self):
        """ëª¨ë¸ì„ GPUì— ë¡œë“œ (4-bit ì–‘ìí™” ì ìš©)"""
        if self._is_loaded:
            logger.info("ëª¨ë¸ì´ ì´ë¯¸ ë¡œë“œë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
            return
        
        model_id = "llava-hf/llava-1.5-7b-hf"
        logger.info("[1/4] LLaVA ë¡œë”© ì‹œì‘: %s", model_id)

        try:
            # 1) Processor
            logger.info("[2/4] Processor ë‹¤ìš´ë¡œë“œ/ë¡œë”© ì‹œì‘")
            t0 = time.time()
            self._processor = AutoProcessor.from_pretrained(model_id)
            logger.info("[2/4] Processor ë¡œë”© ì™„ë£Œ (%.1fs)", time.time() - t0)

            # 3) Quantization config 
            logger.info("[3/4] 4-bit ì–‘ìí™” ì„¤ì • ìƒì„±")
            quantization_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_use_double_quant=True,
                bnb_4bit_compute_dtype=torch.float16
            )
            logger.info("[3/4] ì–‘ìí™” ì„¤ì • ìƒì„± ì™„ë£Œ")

            # 4) Model
            logger.info("[4/4] Model ë‹¤ìš´ë¡œë“œ/ë¡œë”© ì‹œì‘ (ìˆ˜ ë¶„ ê±¸ë¦´ ìˆ˜ ìˆìŒ)")
            t1 = time.time()
            self._model = LlavaForConditionalGeneration.from_pretrained(
                model_id,
                quantization_config=quantization_config,
                device_map="auto",
                torch_dtype=torch.float16,
                low_cpu_mem_usage=True
            )
            logger.info("[4/4] Model ë¡œë”© ì™„ë£Œ (%.1fs)", time.time() - t1)

            self._is_loaded = True
            logger.info("ğŸ‰ ì „ì²´ ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
                
        except Exception as e:
            logger.error(f"ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {str(e)}", exc_info=True)
            raise
    
    def _resize_for_llava(self, image: Image.Image, max_side: int = 672) -> Image.Image:
        """ë„ˆë¬´ í° ì´ë¯¸ì§€ëŠ” ê°•ì œ ì¶•ì†Œí•´ì„œ GPU/ë“œë¼ì´ë²„ ë¦¬ì…‹(TDR) ë°©ì§€"""
        w, h = image.size
        m = max(w, h)
        if m <= max_side:
            return image
        scale = max_side / float(m)
        new_w = int(w * scale)
        new_h = int(h * scale)
        return image.resize((new_w, new_h), Image.BICUBIC)

    def generate_caption(self, image: Image.Image, context: str, temperature: float = 0.7, prompt_variant: int = 1) -> str:
        """
        ì´ë¯¸ì§€ì™€ ë¬¸ë§¥ì„ ê¸°ë°˜ìœ¼ë¡œ ALT í…ìŠ¤íŠ¸ ìƒì„±
        
        Args:
            image: PIL Image ê°ì²´
            context: ë¬¸ë§¥ í…ìŠ¤íŠ¸
            temperature: ìƒì„± ì˜¨ë„ (ë‹¤ì–‘ì„± ì¡°ì ˆ)
            prompt_variant: í”„ë¡¬í”„íŠ¸ ë³€í˜• (1 ë˜ëŠ” 2)
        
        Returns:
            ìƒì„±ëœ ALT í…ìŠ¤íŠ¸
        """
        if not self._is_loaded:
            raise RuntimeError("ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. load()ë¥¼ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.")
        
        # ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì¦ˆ 
        image = self._resize_for_llava(image, max_side=672)  

        context_block = f"Context (supporting hint, do not quote): {context}"
        prompt_common = (
            "Task Context\n"
            "You are an AI assistant that writes ALT text for web accessibility.\n"
            "Your task is to generate a single-sentence ALT text describing the given image.\n"
            "\n"
            "Background Details, Data Documents\n"
            "- Input consists of:\n"
            "  (1) an image\n"
            "  (2) keywords extracted from text written by a human.\n"
            "- The image is the primary source of truth.\n"
            "- The context is only a supporting hint and must NOT be quoted or copied.\n"
            "- Never complete the sentence based primarily on the context.\n"
            "- Use the context only to add nuance (place/situation) IF it matches what is visible.\n"
            "\n"
            "Examples (Few-Shot Prompting)\n"
            "Good examples:\n"
            "- \"ë²„ìŠ¤ ì •ë¥˜ì¥ ê·¼ì²˜ ì¸ë„ì— ë²šê½ƒì´ í•€ ë‚˜ë¬´ê°€ ì¤„ì§€ì–´ ì„œ ìˆë‹¤.\"\n"
            "- \"ë‚˜ë¬´ íƒì ìœ„ì— ë…¸íŠ¸ë¶ì´ ì—´ë ¤ ìˆê³  ì˜†ì— ë¨¸ê·¸ì»µì´ ë†“ì—¬ ìˆë‹¤.\"\n"
            "\n"
            "Bad examples:\n"
            "- Copying or quoting the context text\n"
            "- Adding guesses (e.g., snow, emotions, events) not visually confirmed\n"
            "- Adding headers/labels such as [ë¬¸ë§¥], [ì´ë¯¸ì§€ì„¤ëª…], ALT:, etc.\n"
            "\n"
            "Detailed List of Tasks\n"
            "1. Describe ONLY what is visually observable in the image.\n"
            "2. Use the context only as a minor hint (no quoting), and only within visually confirmed range.\n"
            "3. Write exactly ONE natural Korean sentence.\n"
            "4. Include subject + action/state + background when possible.\n"
            "\n"
            "Important Guidelines\n"
            "- Output must be natural Korean.\n"
            "- No exaggeration, no emotions, no interpretation, no guessing.\n"
            "- Do NOT use speculative phrases like \"~ì¸ ë“¯\", \"~ê°™ë‹¤\", \"ì•„ë§ˆ\".\n"
            "- Do NOT include any meta text, explanations, or labels.\n"
            "- Output ONLY the final ALT sentence.\n"
            "\n"
            "Output Formatting\n"
            "- One Korean sentence only.\n"
            "- No line breaks.\n"
            "- No quotes.\n"
            "- No prefixes.\n"
        )
        
        # í”„ë¡¬í”„íŠ¸ ë³€í˜• (ë‹¤ì–‘í•œ ê´€ì ì˜ ALT ìƒì„±)
        if prompt_variant == 1:
            prompt = (
                f"{prompt_common}"
                "USER: <image>\n"
                f"{context_block}\n"
                "\n"
                "[Follow all the instructions above strictly.]\n"
                "Output requirement: ONE Korean sentence including subject/action(or state)/background.\n"
                "ASSISTANT:"
            )
        else:
            prompt = (
                f"{prompt_common}"
                "USER: <image>\n"
                f"{context_block}\n"
                "\n"
                "[Follow all the instructions above strictly.]\n"
                "Output requirement: ONE Korean sentence, as short and essentially as possible.\n"
                "ASSISTANT:"
            )
                
        # ì…ë ¥ ì²˜ë¦¬ ë° í…ì„œë¥¼ ëª¨ë¸ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
        inputs = self._processor(
            text=prompt,
            images=image,
            return_tensors="pt"
        ).to(self._model.device)
        
        # ìƒì„± íŒŒë¼ë¯¸í„° ì„¤ì • (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ìœ„í•´ ì œí•œ)
        # temperatureê°€ 0.5 ì´ìƒì¼ ë•Œë§Œ do_sample=Trueë¡œ ì„¤ì • (ê·¸ ì™¸ëŠ” greedy decoding)
        use_sampling = temperature >= 0.5
        generation_config = {
            "max_new_tokens": 60,
            "do_sample": use_sampling,
            "temperature": temperature if use_sampling else None,
            "num_beams": 1 if use_sampling else 1,
            "repetition_penalty": 1.1,
            "pad_token_id": self._processor.tokenizer.eos_token_id,
            "eos_token_id": self._processor.tokenizer.eos_token_id,
        }
        # do_sampleì´ Falseì¼ ë•Œ temperature íŒŒë¼ë¯¸í„° ì œê±°
        if not use_sampling:
            generation_config.pop("temperature", None)
        

        try:
        # ì¶”ë¡  ìˆ˜í–‰
            with torch.no_grad():
                generated_ids = self._model.generate(
                    **inputs,
                    **generation_config
                )
        
        
            # ê²°ê³¼ ë””ì½”ë”©
            # ì…ë ¥ í”„ë¡¬í”„íŠ¸ ê¸¸ì´
            prompt_len = inputs["input_ids"].shape[-1]

            # ëª¨ë¸ì´ ìƒˆë¡œ ìƒì„±í•œ í† í°ë§Œ ë¶„ë¦¬
            new_tokens = generated_ids[:, prompt_len:]

            # ë””ì½”ë”©
            generated_text = self._processor.batch_decode(
                new_tokens,
                skip_special_tokens=True,
                clean_up_tokenization_spaces=False
            )[0].strip()

            # ì¤„ë°”ê¿ˆ/ë¼ë²¨ ì œê±° 
            generated_text = generated_text.replace("\n", " ").strip()

            # í•œ ë¬¸ì¥ë§Œ ë‚¨ê¸°ê¸°(ë§ˆì¹¨í‘œ/ë¬¼ìŒí‘œ/ëŠë‚Œí‘œ ê¸°ì¤€ìœ¼ë¡œ ì²« ë¬¸ì¥)
            for sep in ["ã€‚", ".", "!", "?", "ï¼", "ï¼Ÿ"]:
                if sep in generated_text:
                    generated_text = generated_text.split(sep)[0].strip() + ("." if sep == "." else "")
                    break

            return generated_text
        
        except torch.cuda.OutOfMemoryError:
            # âœ… OOM ì•ˆì „ ì²˜ë¦¬
            torch.cuda.empty_cache()
            raise RuntimeError("GPU ë©”ëª¨ë¦¬ ë¶€ì¡±ìœ¼ë¡œ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")  
    
    def _is_alt_similar_to_context(self, alt_text: str, context: str) -> bool:
        """
        ìƒì„±ëœ ALT í…ìŠ¤íŠ¸ê°€ ì‚¬ìš©ì ë¬¸ë§¥ì˜ ì¼ë¶€ì™€ ë™ì¼í•œì§€ í™•ì¸
        
        Args:
            alt_text: ìƒì„±ëœ ALT í…ìŠ¤íŠ¸
            context: ì‚¬ìš©ìê°€ ì‘ì„±í•œ ì›ë³¸ ë¬¸ë§¥ í…ìŠ¤íŠ¸
        
        Returns:
            ALTê°€ ë¬¸ë§¥ì˜ ì¼ë¶€ì™€ ë™ì¼í•˜ë©´ True
        """
        if not alt_text or not context:
            return False
        
        # ê³µë°± ì •ë¦¬
        alt_clean = alt_text.strip()
        context_clean = context.strip()
        
        # ALT í…ìŠ¤íŠ¸ê°€ ë¬¸ë§¥ì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        if alt_clean in context_clean:
            return True
        
        # ALT í…ìŠ¤íŠ¸ì˜ ì£¼ìš” ë¶€ë¶„(ë‹¨ì–´ë“¤)ì´ ë¬¸ë§¥ì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        # ALT í…ìŠ¤íŠ¸ë¥¼ ë‹¨ì–´ë¡œ ë¶„ë¦¬í•˜ì—¬ í™•ì¸
        alt_words = alt_clean.split()
        if len(alt_words) >= 3:  # 3ê°œ ì´ìƒì˜ ë‹¨ì–´ê°€ ìˆìœ¼ë©´
            # ALT í…ìŠ¤íŠ¸ì˜ ì—°ì†ëœ ë‹¨ì–´ë“¤ì´ ë¬¸ë§¥ì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
            for i in range(len(alt_words) - 2):
                phrase = ' '.join(alt_words[i:i+3])  # 3ê°œ ë‹¨ì–´ì”© ë¬¶ì–´ì„œ í™•ì¸
                if phrase in context_clean:
                    return True
        
        return False
    
    def generate_captions(self, image: Image.Image, context: str) -> tuple[str, str]:
        """
        ì´ë¯¸ì§€ì™€ ë¬¸ë§¥ì„ ê¸°ë°˜ìœ¼ë¡œ 2ê°œì˜ ALT í…ìŠ¤íŠ¸ í›„ë³´ ìƒì„±
        
        Args:
            image: PIL Image ê°ì²´
            context: ë¬¸ë§¥ í…ìŠ¤íŠ¸
        
        Returns:
            (ì²« ë²ˆì§¸ ALT, ë‘ ë²ˆì§¸ ALT) íŠœí”Œ
        """
        # ì´ë¯¸ì§€ ë³µì‚¬ë³¸ ìƒì„± (ë‘ ë²ˆì§¸ ìƒì„± ì‹œ ì›ë³¸ ì´ë¯¸ì§€ ì¬ì‚¬ìš© ë°©ì§€)
        import copy
        image_copy = copy.deepcopy(image)
        
        # ì²« ë²ˆì§¸ ALT: ë‚®ì€ temperature (0.2)ë¡œ ì•ˆì •ì ì¸ ìƒì„±, greedy decoding ì‚¬ìš©
        alt1 = self.generate_caption(image, context, temperature=0.2, prompt_variant=1)
        
        # ìƒì„±ëœ ALTê°€ ë¬¸ë§¥ì˜ ì¼ë¶€ì™€ ë™ì¼í•œì§€ í™•ì¸í•˜ê³  ì¬ìƒì„±
        max_context_retries = 3
        context_retry_count = 0
        while self._is_alt_similar_to_context(alt1, context) and context_retry_count < max_context_retries:
            logger.warning(f"ALT 1ì´ ì‚¬ìš©ì ë¬¸ë§¥ê³¼ ìœ ì‚¬í•©ë‹ˆë‹¤. ì¬ìƒì„± ì‹œë„ {context_retry_count + 1}/{max_context_retries}")
            alt1 = self.generate_caption(image, context, temperature=0.3 + (context_retry_count * 0.1), prompt_variant=1)
            context_retry_count += 1
        
        # ë‘ ë²ˆì§¸ ALT: ì²« ë²ˆì§¸ë³´ë‹¤ ì•½ê°„ ë†’ì€ temperature (0.3)ë¡œ ë‹¤ì–‘ì„± ì¦ê°€, ë‹¤ë¥¸ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
        # ì´ë¯¸ì§€ ë³µì‚¬ë³¸ ì‚¬ìš©í•˜ì—¬ ì²« ë²ˆì§¸ ìƒì„±ì˜ ì˜í–¥ ìµœì†Œí™”
        # temperature 0.3ì€ 0.5 ë¯¸ë§Œì´ë¯€ë¡œ greedy decoding ì‚¬ìš©
        alt2 = self.generate_caption(image_copy, context, temperature=0.3, prompt_variant=2)
        
        # ìƒì„±ëœ ALTê°€ ë¬¸ë§¥ì˜ ì¼ë¶€ì™€ ë™ì¼í•œì§€ í™•ì¸í•˜ê³  ì¬ìƒì„±
        context_retry_count = 0
        while self._is_alt_similar_to_context(alt2, context) and context_retry_count < max_context_retries:
            logger.warning(f"ALT 2ê°€ ì‚¬ìš©ì ë¬¸ë§¥ê³¼ ìœ ì‚¬í•©ë‹ˆë‹¤. ì¬ìƒì„± ì‹œë„ {context_retry_count + 1}/{max_context_retries}")
            alt2 = self.generate_caption(image_copy, context, temperature=0.4 + (context_retry_count * 0.1), prompt_variant=2)
            context_retry_count += 1
        
        # ë‘ ALTê°€ ë™ì¼í•œ ê²½ìš° ì¬ìƒì„± ì‹œë„ (ìµœëŒ€ 2íšŒ)
        max_retries = 2
        retry_count = 0
        while alt1 == alt2 and retry_count < max_retries:
            logger.warning(f"ALT 1ê³¼ ALT 2ê°€ ë™ì¼í•©ë‹ˆë‹¤. ì¬ìƒì„± ì‹œë„ {retry_count + 1}/{max_retries}")
            # temperatureë¥¼ ë” ë†’ì—¬ì„œ ì¬ìƒì„±
            alt2 = self.generate_caption(image_copy, context, temperature=0.5, prompt_variant=2)
            retry_count += 1
        
        return (alt1, alt2)
    
    @property
    def model(self):
        """ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
        if not self._is_loaded:
            raise RuntimeError("ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return self._model
    
    @property
    def processor(self):
        """Processor ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
        if not self._is_loaded:
            raise RuntimeError("ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return self._processor


# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
model_loader = LLaVAModelLoader()

