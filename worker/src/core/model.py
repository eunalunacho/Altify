import torch
from transformers import (
    AutoProcessor,
    LlavaForConditionalGeneration,
    BitsAndBytesConfig
)
from PIL import Image
from typing import Optional
import logging
import time

logger = logging.getLogger(__name__)


class LLaVAModelLoader:
    """Singleton íŒ¨í„´ì„ ì‚¬ìš©í•œ LLaVA ëª¨ë¸ ë¡œë”"""
    
    _instance = None
    _model = None
    _processor = None
    _is_loaded = False
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super(LLaVAModelLoader, cls).__new__(cls)
        return cls._instance
    
    def load(self):
        """ëª¨ë¸ì„ GPUì— ë¡œë“œ (4-bit ì–‘ìí™” ì ìš©)"""
        if self._is_loaded:
            logger.info("ëª¨ë¸ì´ ì´ë¯¸ ë¡œë“œë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
            return
        
        model_id = "llava-hf/llava-1.5-7b-hf"
        logger.info("[1/4] LLaVA ë¡œë”© ì‹œì‘: %s", model_id)

        try:
            # 1) Processor
            logger.info("[2/4] Processor ë‹¤ìš´ë¡œë“œ/ë¡œë”© ì‹œì‘")
            t0 = time.time()
            self._processor = AutoProcessor.from_pretrained(model_id)
            logger.info("[2/4] Processor ë¡œë”© ì™„ë£Œ (%.1fs)", time.time() - t0)

            # 2) Quantization config (ì´ê±´ ë¹ ë¦„)
            logger.info("[3/4] 4-bit ì–‘ìí™” ì„¤ì • ìƒì„±")
            quantization_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_use_double_quant=True,
                bnb_4bit_compute_dtype=torch.float16
            )
            logger.info("[3/4] ì–‘ìí™” ì„¤ì • ìƒì„± ì™„ë£Œ")

            # 3) Model (ì—¬ê¸°ê°€ ì˜¤ë˜ ê±¸ë¦¼)
            logger.info("[4/4] Model ë‹¤ìš´ë¡œë“œ/ë¡œë”© ì‹œì‘ (ìˆ˜ ë¶„ ê±¸ë¦´ ìˆ˜ ìˆìŒ)")
            t1 = time.time()
            self._model = LlavaForConditionalGeneration.from_pretrained(
                model_id,
                quantization_config=quantization_config,
                device_map="auto",
                torch_dtype=torch.float16,
                low_cpu_mem_usage=True
            )
            logger.info("[4/4] Model ë¡œë”© ì™„ë£Œ (%.1fs)", time.time() - t1)

            self._is_loaded = True
            logger.info("ğŸ‰ ì „ì²´ ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
                
        except Exception as e:
            logger.error(f"ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {str(e)}", exc_info=True)
            raise
    
    def _resize_for_llava(self, image: Image.Image, max_side: int = 896) -> Image.Image:
        """ë„ˆë¬´ í° ì´ë¯¸ì§€ëŠ” ê°•ì œ ì¶•ì†Œí•´ì„œ GPU/ë“œë¼ì´ë²„ ë¦¬ì…‹(TDR) ë°©ì§€"""
        w, h = image.size
        m = max(w, h)
        if m <= max_side:
            return image
        scale = max_side / float(m)
        new_w = int(w * scale)
        new_h = int(h * scale)
        return image.resize((new_w, new_h), Image.BICUBIC)

    def generate_caption(self, image: Image.Image, context: str, temperature: float = 0.7, prompt_variant: int = 1) -> str:
        """
        ì´ë¯¸ì§€ì™€ ë¬¸ë§¥ì„ ê¸°ë°˜ìœ¼ë¡œ ALT í…ìŠ¤íŠ¸ ìƒì„±
        
        Args:
            image: PIL Image ê°ì²´
            context: ë¬¸ë§¥ í…ìŠ¤íŠ¸
            temperature: ìƒì„± ì˜¨ë„ (ë‹¤ì–‘ì„± ì¡°ì ˆ)
            prompt_variant: í”„ë¡¬í”„íŠ¸ ë³€í˜• (1 ë˜ëŠ” 2)
        
        Returns:
            ìƒì„±ëœ ALT í…ìŠ¤íŠ¸
        """
        if not self._is_loaded:
            raise RuntimeError("ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. load()ë¥¼ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.")
        
        # ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì¦ˆ 
        image = self._resize_for_llava(image, max_side=896)  # 768~1024 ì‚¬ì´ë¡œ ì¡°ì ˆ ì¶”ì²œ

        context_block = f"Context (supporting hint, do not quote): {context}"
        prompt_common = (
            "Task Context\n"
            "You are an AI assistant that writes ALT text for web accessibility.\n"
            "Your task is to generate a single-sentence ALT text describing the given image.\n"
            "\n"
            "Background Details, Data Documents\n"
            "- Input consists of:\n"
            "  (1) an image\n"
            "  (2) an accompanying text (context) written by a human.\n"
            "- The image is the primary source of truth.\n"
            "- The context is only a supporting hint and must NOT be quoted or copied.\n"
            "- Never complete the sentence based primarily on the context.\n"
            "- Use the context only to add nuance (place/situation) IF it matches what is visible.\n"
            "\n"
            "Examples (Few-Shot Prompting)\n"
            "Good examples:\n"
            "- \"ë²„ìŠ¤ ì •ë¥˜ì¥ ê·¼ì²˜ ì¸ë„ì— ë²šê½ƒì´ í•€ ë‚˜ë¬´ê°€ ì¤„ì§€ì–´ ì„œ ìˆë‹¤.\"\n"
            "- \"ë‚˜ë¬´ íƒì ìœ„ì— ë…¸íŠ¸ë¶ì´ ì—´ë ¤ ìˆê³  ì˜†ì— ë¨¸ê·¸ì»µì´ ë†“ì—¬ ìˆë‹¤.\"\n"
            "\n"
            "Bad examples:\n"
            "- Copying or quoting the context text\n"
            "- Adding guesses (e.g., snow, emotions, events) not visually confirmed\n"
            "- Adding headers/labels such as [ë¬¸ë§¥], [ì´ë¯¸ì§€ì„¤ëª…], ALT:, etc.\n"
            "\n"
            "Detailed List of Tasks\n"
            "1. Describe ONLY what is visually observable in the image.\n"
            "2. Use the context only as a minor hint (no quoting), and only within visually confirmed range.\n"
            "3. Write exactly ONE natural Korean sentence.\n"
            "4. Include subject + action/state + background when possible.\n"
            "\n"
            "Important Guidelines\n"
            "- Output must be natural Korean.\n"
            "- No exaggeration, no emotions, no interpretation, no guessing.\n"
            "- Do NOT use speculative phrases like \"~ì¸ ë“¯\", \"~ê°™ë‹¤\", \"ì•„ë§ˆ\".\n"
            "- Do NOT include any meta text, explanations, or labels.\n"
            "- Output ONLY the final ALT sentence.\n"
            "\n"
            "Output Formatting\n"
            "- One Korean sentence only.\n"
            "- No line breaks.\n"
            "- No quotes.\n"
            "- No prefixes.\n"
        )
        
        # í”„ë¡¬í”„íŠ¸ ë³€í˜• (ë‹¤ì–‘í•œ ê´€ì ì˜ ALT ìƒì„±)
        if prompt_variant == 1:
            prompt = (
                f"{prompt_common}"
                "USER: <image>\n"
                f"{context_block}\n"
                "\n"
                "[Follow all the instructions above strictly.]\n"
                "Output requirement: ONE Korean sentence including subject/action(or state)/background.\n"
                "ASSISTANT:"
            )
        else:
            prompt = (
                f"{prompt_common}"
                "USER: <image>\n"
                f"{context_block}\n"
                "\n"
                "[Follow all the instructions above strictly.]\n"
                "Output requirement: ONE Korean sentence, as short and essentially as possible.\n"
                "ASSISTANT:"
            )
                
        # ì…ë ¥ ì²˜ë¦¬ ë° í…ì„œë¥¼ ëª¨ë¸ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
        inputs = self._processor(
            text=prompt,
            images=image,
            return_tensors="pt"
        ).to(self._model.device)
        
        # ìƒì„± íŒŒë¼ë¯¸í„° ì„¤ì • (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ìœ„í•´ ì œí•œ)
        generation_config = {
            "max_new_tokens": 60,
            "do_sample": False,
            "num_beams": 1,
            "repetition_penalty": 1.1,
            "pad_token_id": self._processor.tokenizer.eos_token_id,
            "eos_token_id": self._processor.tokenizer.eos_token_id,
        }
        

        try:
        # ì¶”ë¡  ìˆ˜í–‰
            with torch.no_grad():
                generated_ids = self._model.generate(
                    **inputs,
                    **generation_config
                )
        
        
            # ê²°ê³¼ ë””ì½”ë”©
            # ì…ë ¥ í”„ë¡¬í”„íŠ¸ ê¸¸ì´
            prompt_len = inputs["input_ids"].shape[-1]

            # ëª¨ë¸ì´ ìƒˆë¡œ ìƒì„±í•œ í† í°ë§Œ ë¶„ë¦¬
            new_tokens = generated_ids[:, prompt_len:]

            # ë””ì½”ë”©
            generated_text = self._processor.batch_decode(
                new_tokens,
                skip_special_tokens=True,
                clean_up_tokenization_spaces=False
            )[0].strip()

            # ì¤„ë°”ê¿ˆ/ë¼ë²¨ ì œê±° 
            generated_text = generated_text.replace("\n", " ").strip()

            # í•œ ë¬¸ì¥ë§Œ ë‚¨ê¸°ê¸°(ë§ˆì¹¨í‘œ/ë¬¼ìŒí‘œ/ëŠë‚Œí‘œ ê¸°ì¤€ìœ¼ë¡œ ì²« ë¬¸ì¥)
            for sep in ["ã€‚", ".", "!", "?", "ï¼", "ï¼Ÿ"]:
                if sep in generated_text:
                    generated_text = generated_text.split(sep)[0].strip() + ("." if sep == "." else "")
                    break

            return generated_text
        
        except torch.cuda.OutOfMemoryError:
            # âœ… OOM ì•ˆì „ ì²˜ë¦¬
            torch.cuda.empty_cache()
            raise RuntimeError("GPU ë©”ëª¨ë¦¬ ë¶€ì¡±ìœ¼ë¡œ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")  
    
    def generate_captions(self, image: Image.Image, context: str) -> tuple[str, str]:
        """
        ì´ë¯¸ì§€ì™€ ë¬¸ë§¥ì„ ê¸°ë°˜ìœ¼ë¡œ 2ê°œì˜ ALT í…ìŠ¤íŠ¸ í›„ë³´ ìƒì„±
        
        Args:
            image: PIL Image ê°ì²´
            context: ë¬¸ë§¥ í…ìŠ¤íŠ¸
        
        Returns:
            (ì²« ë²ˆì§¸ ALT, ë‘ ë²ˆì§¸ ALT) íŠœí”Œ
        """
        # ì²« ë²ˆì§¸ ALT: ê¸°ë³¸ temperature (0.7)
        alt1 = self.generate_caption(image, context, temperature=0.7, prompt_variant=1)
        
        # ë‘ ë²ˆì§¸ ALT: ë” ë†’ì€ temperature (0.9)ë¡œ ë‹¤ì–‘ì„± ì¦ê°€, ë‹¤ë¥¸ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
        alt2 = self.generate_caption(image, context, temperature=0.9, prompt_variant=2)
        
        return (alt1, alt2)
    
    @property
    def model(self):
        """ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
        if not self._is_loaded:
            raise RuntimeError("ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return self._model
    
    @property
    def processor(self):
        """Processor ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
        if not self._is_loaded:
            raise RuntimeError("ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return self._processor


# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
model_loader = LLaVAModelLoader()

